{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOPE\n",
    "---\n",
    "\n",
    "With `dope`, our goal is to make all existing standard machine learning frameworks(say sklearn, suprislib, pytorch, tensorflow etc) interoperable. That is, one can devlop and train a model, say, using Linear Regression in sklearn, and score it using a TensorFlow server.\n",
    "\n",
    "In this tutorial, we walk through an example demonstrating one such scenario.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "---\n",
    "Setting the context(Terminologies used) -  \n",
    "1) Primal model - Primal model refers to the base model provided by the user. For example, the primal model in the scenario demonstrated below would be the `LogisticRegression()` class instance from sklearn.  \n",
    "2) dope - The dope function converts your primal model to it's dnn equivalent. Also, dope ensures that the functional and behavioural aspects of your primal model is retained when it's \"dope\"d.\n",
    "\n",
    "\n",
    "*Note - The usage of `dope` is pretty straightforward as long as the user has a decent understanding of basic Sklearn and Keras functionalities.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading and preprocessing dataset\n",
    "---\n",
    "In this example we will use the iris dataset. The primal model used here is sklearn's Logistic Regression class. The `dope` function converts sklearn's Logistic Regression model to it's Neural Network equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "# Split the data in to test and train batches\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Instantiate the primal model\n",
    "---\n",
    "Instantiate the model you wish to convert in to a Neural network. Here, we use sklearn's logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: \"Dope\" your primal model!\n",
    "---\n",
    "The `dope` function lets you convert your primal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpiling your model to it's Deep Neural Network equivalent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-04-07 12:58:30,739\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-07_12-58-30_2179/logs.\n",
      "2019-04-07 12:58:30,849\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:53332 to respond...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-07 12:58:30,971\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:50444 to respond...\n",
      "2019-04-07 12:58:30,973\tINFO services.py:760 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-04-07 12:58:31,005\tINFO services.py:1384 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n",
      "2019-04-07 12:58:31,023\tWARNING services.py:863 -- Failed to start the reporter. The reporter requires 'pip install psutil'.\n"
     ]
    }
   ],
   "source": [
    "from mlsquare import dope\n",
    "m = dope(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Note - The warning message you see about redis server is a part of the optimization process `dope` does. The details about this will be covered in the upcoming tutorials(yet to be published). So fret not! These warning messages can be safely ignored.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Voila! You have successfully Doped your model\n",
    "---\n",
    "Once you have successfully run the `dope` function by passing your primal model, the returned model(the variable `m` here) would behave like any other sklearn models. The only difference being that the model is not a standard sklearn model but a dnn equivalent of the model provided by you.\n",
    "\n",
    "The below mentioned methods demonstrate the resemblance of an \"dope'd\" model with sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_env36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_env36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fbd86c5f748>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit your model ##\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 254us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3666666699780358"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Score your model ##\n",
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save your model ##\n",
    "m.save('demo_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note - The save method expects a single argument - filename. You will be able to find the saved model in the directory you're running your script from. The model by default is saved in three formats - h5, onnx and a serialized pickle file.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
