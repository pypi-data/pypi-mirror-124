
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_history.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_history.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_history.py:


Trace applied transforms
========================

Sometimes we would like to see which transform was applied to a certain batch
during training. This can be done in TorchIO using
:func:`torchio.utils.history_collate` for the data loader. The transforms
history can be saved during training to check what was applied.

.. GENERATED FROM PYTHON SOURCE LINES 10-62



.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /auto_examples/images/sphx_glr_plot_history_001.png
          :alt: ToCanonical, Gamma, Blur, Flip, RescaleIntensity, Sagittal, Coronal, Axial
          :class: sphx-glr-multi-img

    *

      .. image:: /auto_examples/images/sphx_glr_plot_history_002.png
          :alt: ToCanonical, Blur, RescaleIntensity, Sagittal, Coronal, Axial
          :class: sphx-glr-multi-img

    *

      .. image:: /auto_examples/images/sphx_glr_plot_history_003.png
          :alt: ToCanonical, Gamma, Flip, RescaleIntensity, Sagittal, Coronal, Axial
          :class: sphx-glr-multi-img

    *

      .. image:: /auto_examples/images/sphx_glr_plot_history_004.png
          :alt: ToCanonical, Gamma, Blur, Flip, RescaleIntensity, Sagittal, Coronal, Axial
          :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Applied transforms:
    [ToCanonical(),
     Gamma(gamma={'t1': [0.8018916845321655]}),
     RescaleIntensity(out_min_max=(-1, 1), percentiles=(0, 100), masking_method=None)]

    Composed transform to reproduce history:
    Compose([ToCanonical(), Gamma(gamma={'t1': [0.8018916845321655]}), RescaleIntensity(out_min_max=(-1, 1), percentiles=(0, 100), masking_method=None)])

    Composed transform to invert applied transforms when possible:
    /home/fernando/git/torchio/torchio/transforms/augmentation/composition.py:65: RuntimeWarning: Skipping ToCanonical as it is not invertible
      warnings.warn(message, RuntimeWarning)
    /home/fernando/git/torchio/torchio/transforms/augmentation/composition.py:65: RuntimeWarning: Skipping RescaleIntensity as it is not invertible
      warnings.warn(message, RuntimeWarning)
    Compose([Gamma(gamma={'t1': [0.8018916845321655]}, invert=True)])

    Transforms applied to subjects in batch:
    [[ToCanonical(),
      Gamma(gamma={'t1': [1.125920057296753]}),
      Blur(std={'t1': [tensor([0.5645, 1.3632, 1.8304])]}),
      Flip(axes=(0,)),
      RescaleIntensity(out_min_max=(-1, 1), percentiles=(0, 100), masking_method=None)],
     [ToCanonical(),
      Blur(std={'t1': [tensor([0.5397, 0.3014, 0.0634])]}),
      RescaleIntensity(out_min_max=(-1, 1), percentiles=(0, 100), masking_method=None)],
     [ToCanonical(),
      Gamma(gamma={'t1': [0.8567072749137878]}),
      Flip(axes=(0,)),
      RescaleIntensity(out_min_max=(-1, 1), percentiles=(0, 100), masking_method=None)],
     [ToCanonical(),
      Gamma(gamma={'t1': [0.7924771308898926]}),
      Blur(std={'t1': [tensor([1.4525, 1.4022, 0.4076])]}),
      Flip(axes=(0,)),
      RescaleIntensity(out_min_max=(-1, 1), percentiles=(0, 100), masking_method=None)]]






|

.. code-block:: default


    import pprint
    import torch
    import torchio as tio
    import matplotlib.pyplot as plt

    torch.manual_seed(0)

    batch_size = 4
    subject = tio.datasets.FPG()
    subject.remove_image('seg')
    subjects = 4 * [subject]

    transform = tio.Compose((
        tio.ToCanonical(),
        tio.RandomGamma(p=0.75),
        tio.RandomBlur(p=0.5),
        tio.RandomFlip(),
        tio.RescaleIntensity(out_min_max=(-1, 1)),
    ))

    dataset = tio.SubjectsDataset(subjects, transform=transform)

    transformed = dataset[0]
    print('Applied transforms:')  # noqa: T001
    pprint.pprint(transformed.history)  # noqa: T003
    print('\nComposed transform to reproduce history:')  # noqa: T001
    print(transformed.get_composed_history())  # noqa: T001
    print('\nComposed transform to invert applied transforms when possible:')  # noqa: T001, E501
    print(transformed.get_inverse_transform(ignore_intensity=False))  # noqa: T001

    loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        collate_fn=tio.utils.history_collate,
    )

    batch = tio.utils.get_first_item(loader)
    print('\nTransforms applied to subjects in batch:')  # noqa: T001
    pprint.pprint(batch[tio.HISTORY])  # noqa: T003

    for i in range(batch_size):
        tensor = batch['t1'][tio.DATA][i]
        affine = batch['t1'][tio.AFFINE][i]
        image = tio.ScalarImage(tensor=tensor, affine=affine)
        image.plot(show=False)
        history = batch[tio.HISTORY][i]
        title = ', '.join(t.name for t in history)
        plt.suptitle(title)
        plt.tight_layout()

    plt.show()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  20.938 seconds)


.. _sphx_glr_download_auto_examples_plot_history.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/fepegar/torchio/master?filepath=notebooks/auto_examples/plot_history.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_history.py <plot_history.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_history.ipynb <plot_history.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
